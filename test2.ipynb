{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.models import Sequential\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing import sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 50000\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=vocab_size)\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'as', 'you', 'with', 'out', 'themselves', 'powerful', 'lets', 'loves', 'their', 'becomes', 'reaching', 'had', 'journalist', 'of', 'lot', 'from', 'anyone', 'to', 'have', 'after', 'out', 'atmosphere', 'never', 'more', 'room', 'titillate', 'it', 'so', 'heart', 'shows', 'to', 'years', 'of', 'every', 'never', 'going', 'villaronga', 'help', 'moments', 'or', 'of', 'every', 'chest', 'visual', 'movie', 'except', 'her', 'was', 'several', 'of', 'enough', 'more', 'with', 'is', 'now', 'current', 'film', 'as', 'you', 'of', 'mine', 'potentially', 'unfortunately', 'of', 'you', 'than', 'him', 'that', 'with', 'out', 'themselves', 'her', 'get', 'for', 'was', 'camp', 'of', 'you', 'movie', 'sometimes', 'movie', 'that', 'with', 'scary', 'but', 'pratfalls', 'to', 'story', 'wonderful', 'that', 'in', 'seeing', 'in', 'character', 'to', 'of', '70s', 'musicians', 'with', 'heart', 'had', 'shadows', 'they', 'of', 'here', 'that', 'with', 'her', 'serious', 'to', 'have', 'does', 'when', 'from', 'why', 'what', 'have', 'critics', 'they', 'is', 'you', 'that', \"isn't\", 'one', 'will', 'very', 'to', 'as', 'itself', 'with', 'other', 'tricky', 'in', 'of', 'seen', 'over', 'landed', 'for', 'anyone', 'of', \"gilmore's\", 'br', \"show's\", 'to', 'whether', 'from', 'than', 'out', 'themselves', 'history', 'he', 'name', 'half', 'some', 'br', 'of', \"'n\", 'odd', 'was', 'two', 'most', 'of', 'mean', 'for', '1', 'any', 'an', 'boat', 'she', 'he', 'should', 'is', 'thought', 'frog', 'but', 'of', 'script', 'you', 'not', 'while', 'history', 'he', 'heart', 'to', 'real', 'at', 'barrel', 'but', 'when', 'from', 'one', 'bit', 'then', 'have', 'two', 'of', 'script', 'their', 'with', 'her', 'nobody', 'most', 'that', 'with', \"wasn't\", 'to', 'with', 'armed', 'acting', 'watch', 'an', 'for', 'with', 'heartfelt', 'film', 'want', 'an']\n"
     ]
    }
   ],
   "source": [
    "word_idx = imdb.get_word_index()\n",
    " \n",
    "# Originally the index number of a value and not a key,\n",
    "# hence converting the index as key and the words as values\n",
    "word_idx = {i: word for word, i in word_idx.items()}\n",
    " \n",
    "# again printing the review\n",
    "print([word_idx[i] for i in train_data[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Review:\n",
      "the as you with out themselves powerful lets loves their becomes reaching had journalist of lot from anyone to have after out atmosphere never more room titillate it so heart shows to years of every never going villaronga help moments or of every chest visual movie except her was several of enough more with is now current film as you of mine potentially unfortunately of you than him that with out themselves her get for was camp of you movie sometimes movie that with scary but pratfalls to story wonderful that in seeing in character to of 70s musicians with heart had shadows they of here that with her serious to have does when from why what have critics they is you that isn't one will very to as itself with other tricky in of seen over landed for anyone of gilmore's br show's to whether from than out themselves history he name half some br of 'n odd was two most of mean for 1 any an boat she he should is thought frog but of script you not while history he heart to real at barrel but when from one bit then have two of script their with her nobody most that with wasn't to with armed acting watch an for with heartfelt film want an\n"
     ]
    }
   ],
   "source": [
    "train_texts = [' '.join([word_idx.get(i, '') for i in seq]) for seq in train_data]\n",
    "test_texts = [' '.join([word_idx.get(i, '') for i in seq]) for seq in test_data]\n",
    "\n",
    "# Display the original review\n",
    "print(\"Original Review:\")\n",
    "print(train_texts[0])\n",
    "\n",
    "# Convert the text data to sequences using the Tokenizer\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(train_texts)\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length of review:   2697\n",
      "Min length of a review:   70\n"
     ]
    }
   ],
   "source": [
    "print(\"Max length of review:  \", len(max((train_data+test_data), key=len)))\n",
    "print(\"Min length of a review:  \", len(min((train_data+test_data), key=len)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "max_words = 400\n",
    "\n",
    "train_data = sequence.pad_sequences(train_sequences, maxlen=max_words)\n",
    "test_data = sequence.pad_sequences(test_sequences, maxlen=max_words)\n",
    "\n",
    "x_valid, y_valid = train_data[:64], train_labels[:64]\n",
    "train_data_, train_labels_ = train_data[64:], train_labels[64:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Simple_RNN\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 400, 32)           1600000   \n",
      "                                                                 \n",
      " simple_rnn_1 (SimpleRNN)    (None, 128)               20608     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1620737 (6.18 MB)\n",
      "Trainable params: 1620737 (6.18 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embd_len = 32\n",
    "\n",
    "RNN_model = Sequential(name=\"Simple_RNN\")\n",
    "RNN_model.add(layers.Embedding(vocab_size, embd_len, input_length=max_words))\n",
    "\n",
    "RNN_model.add(layers.SimpleRNN(128, activation='tanh', return_sequences=False))\n",
    "\n",
    "RNN_model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "print(RNN_model.summary())\n",
    "\n",
    "RNN_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "390/390 [==============================] - 19s 50ms/step - loss: 0.3884 - accuracy: 0.8308 - val_loss: 0.5517 - val_accuracy: 0.7656\n",
      "Epoch 2/20\n",
      "390/390 [==============================] - 20s 52ms/step - loss: 0.2748 - accuracy: 0.8936 - val_loss: 0.5569 - val_accuracy: 0.7500\n",
      "Epoch 3/20\n",
      "390/390 [==============================] - 22s 55ms/step - loss: 0.2167 - accuracy: 0.9214 - val_loss: 0.5887 - val_accuracy: 0.7500\n",
      "Epoch 4/20\n",
      "390/390 [==============================] - 22s 55ms/step - loss: 0.1834 - accuracy: 0.9359 - val_loss: 0.5765 - val_accuracy: 0.7812\n",
      "Epoch 5/20\n",
      "390/390 [==============================] - 22s 56ms/step - loss: 0.1420 - accuracy: 0.9540 - val_loss: 0.5468 - val_accuracy: 0.7969\n",
      "Epoch 6/20\n",
      "390/390 [==============================] - 22s 56ms/step - loss: 0.1188 - accuracy: 0.9629 - val_loss: 0.5991 - val_accuracy: 0.7969\n",
      "Epoch 7/20\n",
      "390/390 [==============================] - 22s 56ms/step - loss: 0.0934 - accuracy: 0.9721 - val_loss: 0.5736 - val_accuracy: 0.8125\n",
      "Epoch 8/20\n",
      "390/390 [==============================] - 22s 57ms/step - loss: 0.0875 - accuracy: 0.9726 - val_loss: 0.6433 - val_accuracy: 0.8438\n",
      "Epoch 9/20\n",
      "390/390 [==============================] - 32s 81ms/step - loss: 0.0799 - accuracy: 0.9754 - val_loss: 0.5458 - val_accuracy: 0.7812\n",
      "Epoch 10/20\n",
      "390/390 [==============================] - 68s 174ms/step - loss: 0.6098 - accuracy: 0.6970 - val_loss: 0.6457 - val_accuracy: 0.6250\n",
      "Epoch 11/20\n",
      "390/390 [==============================] - 28s 72ms/step - loss: 0.4638 - accuracy: 0.7790 - val_loss: 0.6153 - val_accuracy: 0.6562\n",
      "Epoch 12/20\n",
      "390/390 [==============================] - 44s 112ms/step - loss: 0.4048 - accuracy: 0.8260 - val_loss: 0.6214 - val_accuracy: 0.5938\n",
      "Epoch 13/20\n",
      "390/390 [==============================] - 21s 53ms/step - loss: 0.4196 - accuracy: 0.8062 - val_loss: 0.7027 - val_accuracy: 0.7031\n",
      "Epoch 14/20\n",
      "390/390 [==============================] - 37s 95ms/step - loss: 0.2891 - accuracy: 0.8899 - val_loss: 0.7415 - val_accuracy: 0.7188\n",
      "Epoch 15/20\n",
      "390/390 [==============================] - 22s 58ms/step - loss: 0.3904 - accuracy: 0.8231 - val_loss: 0.6262 - val_accuracy: 0.7344\n",
      "Epoch 16/20\n",
      "390/390 [==============================] - 22s 57ms/step - loss: 0.4158 - accuracy: 0.8069 - val_loss: 0.7295 - val_accuracy: 0.6250\n",
      "Epoch 17/20\n",
      "390/390 [==============================] - 22s 57ms/step - loss: 0.4094 - accuracy: 0.8011 - val_loss: 0.7910 - val_accuracy: 0.7344\n",
      "Epoch 18/20\n",
      "390/390 [==============================] - 23s 59ms/step - loss: 0.2738 - accuracy: 0.8919 - val_loss: 0.7275 - val_accuracy: 0.7500\n",
      "Epoch 19/20\n",
      "390/390 [==============================] - 23s 58ms/step - loss: 0.2850 - accuracy: 0.8876 - val_loss: 0.6875 - val_accuracy: 0.6719\n",
      "Epoch 20/20\n",
      "390/390 [==============================] - 22s 57ms/step - loss: 0.2774 - accuracy: 0.8880 - val_loss: 1.0169 - val_accuracy: 0.7031\n",
      "\n",
      "Simple_RNN Score --->  [0.7244879603385925, 0.7017599940299988]\n"
     ]
    }
   ],
   "source": [
    "analysis = RNN_model.fit(train_data_,train_labels_, batch_size=64, epochs=20, verbose=1, validation_data=(x_valid, y_valid))\n",
    "print()\n",
    "print(\"Simple_RNN Score ---> \", RNN_model.evaluate(test_data, test_labels, verbose=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 26ms/step\n",
      "Sentiment Prediction: [[0.99995506]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nif(prediction < 0.4):\\n    predicted_label = 0\\nelif(prediction >= 0.4 and prediction < 0.7):\\n    predicted_label = 1\\nelse:\\n    predicted_label = 2\\n\\n\\n\\n# Display the binary label\\nprint(\"Predicted Label:\", predicted_label)'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_text = \"This movie was a complete disappointment. The plot was confusing, the characters were poorly developed, and the acting was terrible. I regret wasting my time and money on this film. Definitely not recommended.\"\n",
    "\n",
    "\n",
    "\n",
    "# Preprocess the new text\n",
    "new_text_sequence = tokenizer.texts_to_sequences([new_text])\n",
    "new_text_padded = sequence.pad_sequences(new_text_sequence, maxlen=max_words)\n",
    "\n",
    "# Make the prediction\n",
    "prediction = RNN_model.predict(new_text_padded)\n",
    "\n",
    "# Display the prediction\n",
    "print(\"Sentiment Prediction:\", prediction)\n",
    "\n",
    "# Convert the prediction to label 0(negative), 1(neutral), 2(positive)\n",
    "#predicted_label = 1 if prediction > 0.4 else 0\n",
    "\n",
    "'''\n",
    "if(prediction < 0.4):\n",
    "    predicted_label = 0\n",
    "elif(prediction >= 0.4 and prediction < 0.7):\n",
    "    predicted_label = 1\n",
    "else:\n",
    "    predicted_label = 2\n",
    "\n",
    "\n",
    "\n",
    "# Display the binary label\n",
    "print(\"Predicted Label:\", predicted_label)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
