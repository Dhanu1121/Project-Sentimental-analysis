{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./preprocessed_twitter_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500000, 3)\n",
      "        Unnamed: 0                                              tweet  \\\n",
      "0                0  upset updat facebook text might cri result sch...   \n",
      "1                1       dive mani time ball manag save rest go bound   \n",
      "2                2                    whole bodi feel itchi like fire   \n",
      "3                3                                      behav mad see   \n",
      "4                4                                         whole crew   \n",
      "...            ...                                                ...   \n",
      "499995      499995                      idea use web p tweetdeck work   \n",
      "499996      499996          ww work gain back probabl put chip anyway   \n",
      "499997      499997                                   sorri mommi miss   \n",
      "499998      499998     terribl headach last night weight decreas aaah   \n",
      "499999      499999                                         cant sleep   \n",
      "\n",
      "        sentiment  \n",
      "0               0  \n",
      "1               0  \n",
      "2               0  \n",
      "3               0  \n",
      "4               0  \n",
      "...           ...  \n",
      "499995          0  \n",
      "499996          0  \n",
      "499997          0  \n",
      "499998          0  \n",
      "499999          0  \n",
      "\n",
      "[500000 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "negative_tweets = df[df['sentiment'] == 0].head(500000)\n",
    "print(negative_tweets.shape)\n",
    "print(negative_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500000, 3)\n",
      "         Unnamed: 0                                              tweet  \\\n",
      "799999       799999                                  love u guy r best   \n",
      "800000       800000      im meet one besti tonight cant wait girl talk   \n",
      "800001       800001  thank twitter add sunisa got meet hin show dc ...   \n",
      "800002       800002  sick realli cheap hurt much eat real food plu ...   \n",
      "800003       800003                                     effect everyon   \n",
      "...             ...                                                ...   \n",
      "1299994     1299994                      watch horton hear tasha apart   \n",
      "1299995     1299995      r u friendfe got friend request u sure realli   \n",
      "1299996     1299996  thank comment grace record toast ice americano...   \n",
      "1299997     1299997  look launch uniqu art web store uniqu art shir...   \n",
      "1299998     1299998  new blog potenti cake diaster ahhhh www mandic...   \n",
      "\n",
      "         sentiment  \n",
      "799999           1  \n",
      "800000           1  \n",
      "800001           1  \n",
      "800002           1  \n",
      "800003           1  \n",
      "...            ...  \n",
      "1299994          1  \n",
      "1299995          1  \n",
      "1299996          1  \n",
      "1299997          1  \n",
      "1299998          1  \n",
      "\n",
      "[500000 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "positive_tweets = df[df['sentiment'] == 1].head(500000)\n",
    "print(positive_tweets.shape)\n",
    "print(positive_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 3)\n",
      "         Unnamed: 0                                              tweet  \\\n",
      "3123980     3123980                              top tablet damn right   \n",
      "3123981     3123981              cnbctv appl margin better expect aapl   \n",
      "3123987     3123987  rt bought store pretti good logo match wait in...   \n",
      "3124000     3124002  latest appl product lead effici iphon ipad plu...   \n",
      "3124005     3124007                              rt thank think upgrad   \n",
      "...             ...                                                ...   \n",
      "3141591     3141673   better anyon els busi stock invest tradeidea gnu   \n",
      "3141592     3141674  rt sound eerili like famou julian robertson un...   \n",
      "3141593     3141675  arent extrem optimist yet market go spx spi iw...   \n",
      "3141594     3141676  case covid florida past week still set open pa...   \n",
      "3141595     3141677  investor take chanc biogen stock biib rhhbi nv...   \n",
      "\n",
      "         sentiment  \n",
      "3123980          2  \n",
      "3123981          2  \n",
      "3123987          2  \n",
      "3124000          2  \n",
      "3124005          2  \n",
      "...            ...  \n",
      "3141591          2  \n",
      "3141592          2  \n",
      "3141593          2  \n",
      "3141594          2  \n",
      "3141595          2  \n",
      "\n",
      "[10000 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "neutral_tweets = df[df['sentiment'] == 2].head(10000)\n",
    "print(neutral_tweets.shape)\n",
    "print(neutral_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split negative_tweets into train and test sets\n",
    "train_negative_tweets, test_negative_tweets = train_test_split(negative_tweets, test_size=0.3, random_state=42)\n",
    "\n",
    "# Split positive_tweets into train and test sets\n",
    "train_positive_tweets, test_positive_tweets = train_test_split(positive_tweets, test_size=0.3, random_state=42)\n",
    "\n",
    "# Split neutral_tweets into train and test sets\n",
    "train_neutral_tweets, test_neutral_tweets = train_test_split(neutral_tweets, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\Final Year Project\\Project(latest)\\.venv\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Maxlen:  56\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Combine the train sets for all sentiment categories and shuffle\n",
    "train_tweets = shuffle(pd.concat([train_negative_tweets, train_positive_tweets, train_neutral_tweets], ignore_index=True), random_state=42)\n",
    "\n",
    "# Combine the test sets for all sentiment categories and shuffle\n",
    "test_tweets = shuffle(pd.concat([test_negative_tweets, test_positive_tweets, test_neutral_tweets], ignore_index=True), random_state=42)\n",
    "\n",
    "# Handle NaN values in the 'text' column\n",
    "train_tweets = train_tweets.dropna(subset=['tweet'])\n",
    "test_tweets = test_tweets.dropna(subset=['tweet'])\n",
    "\n",
    "# Get the normalized text reviews from the combined train and test sets\n",
    "tweet_train_reviews = train_tweets['tweet'].tolist()\n",
    "tweet_test_reviews = test_tweets['tweet'].tolist()\n",
    "\n",
    "# Train lables and test lables of tweets\n",
    "train_labels = train_tweets['sentiment']\n",
    "test_labels = test_tweets['sentiment']\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# Fit the tokenizer on the training data\n",
    "tokenizer.fit_on_texts(tweet_train_reviews)\n",
    "\n",
    "# Convert text to sequences\n",
    "train_sequences = tokenizer.texts_to_sequences(tweet_train_reviews)\n",
    "test_sequences = tokenizer.texts_to_sequences(tweet_test_reviews)\n",
    "\n",
    "# Ensure sequences are padded to the same length\n",
    "maxlen = max(len(seq) for seq in train_sequences + test_sequences)\n",
    "print(\"Maxlen: \" , maxlen)\n",
    "train_sequences = pad_sequences(train_sequences, maxlen=maxlen)\n",
    "test_sequences = pad_sequences(test_sequences, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\Final Year Project\\Project(latest)\\.venv\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\Final Year Project\\Project(latest)\\.venv\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/5\n",
      "WARNING:tensorflow:From d:\\Final Year Project\\Project(latest)\\.venv\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\Final Year Project\\Project(latest)\\.venv\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "8797/8797 [==============================] - 1192s 135ms/step - loss: 0.4915 - accuracy: 0.7662 - val_loss: 0.4720 - val_accuracy: 0.7756\n",
      "Epoch 2/5\n",
      "8797/8797 [==============================] - 952s 108ms/step - loss: 0.4275 - accuracy: 0.8037 - val_loss: 0.4731 - val_accuracy: 0.7764\n",
      "Epoch 3/5\n",
      "8797/8797 [==============================] - 954s 108ms/step - loss: 0.3745 - accuracy: 0.8322 - val_loss: 0.4999 - val_accuracy: 0.7690\n",
      "Epoch 4/5\n",
      "8797/8797 [==============================] - 954s 108ms/step - loss: 0.3290 - accuracy: 0.8540 - val_loss: 0.5303 - val_accuracy: 0.7645\n",
      "Epoch 5/5\n",
      "8797/8797 [==============================] - 931s 106ms/step - loss: 0.2923 - accuracy: 0.8704 - val_loss: 0.5861 - val_accuracy: 0.7516\n",
      "9424/9424 [==============================] - 62s 7ms/step - loss: 0.5789 - accuracy: 0.7580\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5788764953613281, 0.757967472076416]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# Assuming you have already defined vocab_size, embedding_dim, maxlen, train_sequences, and train_labels\n",
    "# ...\n",
    "vocab_size = len(tokenizer.word_index) + 1  # add 1 for the padding token\n",
    "embedding_dim = 100  # adjust as needed\n",
    "maxlen = maxlen  # as defined earlier\n",
    "train_sequences = train_sequences  # as defined earlier\n",
    "train_labels = train_labels  # as defined earlier\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=maxlen))\n",
    "model.add(LSTM(units=100))\n",
    "model.add(Dense(units=3, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "# Train the model\n",
    "model.fit(train_sequences, train_labels, epochs=5, batch_size=64, validation_split=0.2)\n",
    "model.evaluate(test_sequences, test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('model.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
