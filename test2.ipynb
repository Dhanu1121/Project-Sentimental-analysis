{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.models import Sequential\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing import sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 2, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 2, 19, 178, 32]\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 5000\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=vocab_size)\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'as', 'you', 'with', 'out', 'themselves', 'powerful', 'lets', 'loves', 'their', 'becomes', 'reaching', 'had', 'journalist', 'of', 'lot', 'from', 'anyone', 'to', 'have', 'after', 'out', 'atmosphere', 'never', 'more', 'room', 'and', 'it', 'so', 'heart', 'shows', 'to', 'years', 'of', 'every', 'never', 'going', 'and', 'help', 'moments', 'or', 'of', 'every', 'chest', 'visual', 'movie', 'except', 'her', 'was', 'several', 'of', 'enough', 'more', 'with', 'is', 'now', 'current', 'film', 'as', 'you', 'of', 'mine', 'potentially', 'unfortunately', 'of', 'you', 'than', 'him', 'that', 'with', 'out', 'themselves', 'her', 'get', 'for', 'was', 'camp', 'of', 'you', 'movie', 'sometimes', 'movie', 'that', 'with', 'scary', 'but', 'and', 'to', 'story', 'wonderful', 'that', 'in', 'seeing', 'in', 'character', 'to', 'of', '70s', 'and', 'with', 'heart', 'had', 'shadows', 'they', 'of', 'here', 'that', 'with', 'her', 'serious', 'to', 'have', 'does', 'when', 'from', 'why', 'what', 'have', 'critics', 'they', 'is', 'you', 'that', \"isn't\", 'one', 'will', 'very', 'to', 'as', 'itself', 'with', 'other', 'and', 'in', 'of', 'seen', 'over', 'and', 'for', 'anyone', 'of', 'and', 'br', \"show's\", 'to', 'whether', 'from', 'than', 'out', 'themselves', 'history', 'he', 'name', 'half', 'some', 'br', 'of', 'and', 'odd', 'was', 'two', 'most', 'of', 'mean', 'for', '1', 'any', 'an', 'boat', 'she', 'he', 'should', 'is', 'thought', 'and', 'but', 'of', 'script', 'you', 'not', 'while', 'history', 'he', 'heart', 'to', 'real', 'at', 'and', 'but', 'when', 'from', 'one', 'bit', 'then', 'have', 'two', 'of', 'script', 'their', 'with', 'her', 'nobody', 'most', 'that', 'with', \"wasn't\", 'to', 'with', 'armed', 'acting', 'watch', 'an', 'for', 'with', 'and', 'film', 'want', 'an']\n"
     ]
    }
   ],
   "source": [
    "word_idx = imdb.get_word_index()\n",
    " \n",
    "# Originally the index number of a value and not a key,\n",
    "# hence converting the index as key and the words as values\n",
    "word_idx = {i: word for word, i in word_idx.items()}\n",
    " \n",
    "# again printing the review\n",
    "print([word_idx[i] for i in train_data[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Review:\n",
      "the as you with out themselves powerful lets loves their becomes reaching had journalist of lot from anyone to have after out atmosphere never more room and it so heart shows to years of every never going and help moments or of every chest visual movie except her was several of enough more with is now current film as you of mine potentially unfortunately of you than him that with out themselves her get for was camp of you movie sometimes movie that with scary but and to story wonderful that in seeing in character to of 70s and with heart had shadows they of here that with her serious to have does when from why what have critics they is you that isn't one will very to as itself with other and in of seen over and for anyone of and br show's to whether from than out themselves history he name half some br of and odd was two most of mean for 1 any an boat she he should is thought and but of script you not while history he heart to real at and but when from one bit then have two of script their with her nobody most that with wasn't to with armed acting watch an for with and film want an\n"
     ]
    }
   ],
   "source": [
    "train_texts = [' '.join([word_idx.get(i, '') for i in seq]) for seq in train_data]\n",
    "test_texts = [' '.join([word_idx.get(i, '') for i in seq]) for seq in test_data]\n",
    "\n",
    "# Display the original review\n",
    "print(\"Original Review:\")\n",
    "print(train_texts[0])\n",
    "\n",
    "# Convert the text data to sequences using the Tokenizer\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(train_texts)\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length of review:   2697\n",
      "Min length of a review:   70\n"
     ]
    }
   ],
   "source": [
    "print(\"Max length of review:  \", len(max((train_data+test_data), key=len)))\n",
    "print(\"Min length of a review:  \", len(min((train_data+test_data), key=len)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "max_words = 400\n",
    "\n",
    "train_data = sequence.pad_sequences(train_sequences, maxlen=max_words)\n",
    "test_data = sequence.pad_sequences(test_sequences, maxlen=max_words)\n",
    "\n",
    "x_valid, y_valid = train_data[:64], train_labels[:64]\n",
    "train_data_, train_labels_ = train_data[64:], train_labels[64:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Simple_RNN\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 400, 32)           160000    \n",
      "                                                                 \n",
      " simple_rnn_1 (SimpleRNN)    (None, 128)               20608     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 180737 (706.00 KB)\n",
      "Trainable params: 180737 (706.00 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embd_len = 32\n",
    "\n",
    "RNN_model = Sequential(name=\"Simple_RNN\")\n",
    "RNN_model.add(layers.Embedding(vocab_size, embd_len, input_length=max_words))\n",
    "\n",
    "RNN_model.add(layers.SimpleRNN(128, activation='tanh', return_sequences=False))\n",
    "\n",
    "RNN_model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "print(RNN_model.summary())\n",
    "\n",
    "RNN_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.6893 - accuracy: 0.5371 - val_loss: 0.6760 - val_accuracy: 0.5938\n",
      "Epoch 2/5\n",
      "390/390 [==============================] - 17s 45ms/step - loss: 0.6285 - accuracy: 0.6364 - val_loss: 0.6613 - val_accuracy: 0.6562\n",
      "Epoch 3/5\n",
      "390/390 [==============================] - 18s 46ms/step - loss: 0.5060 - accuracy: 0.7575 - val_loss: 0.6008 - val_accuracy: 0.6719\n",
      "Epoch 4/5\n",
      "390/390 [==============================] - 19s 50ms/step - loss: 0.4091 - accuracy: 0.8211 - val_loss: 0.4409 - val_accuracy: 0.8125\n",
      "Epoch 5/5\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.4485 - accuracy: 0.8025 - val_loss: 0.3969 - val_accuracy: 0.8438\n",
      "\n",
      "Simple_RNN Score --->  [0.4469980001449585, 0.800279974937439]\n"
     ]
    }
   ],
   "source": [
    "analysis = RNN_model.fit(train_data_,train_labels_, batch_size=64, epochs=5, verbose=1, validation_data=(x_valid, y_valid))\n",
    "print()\n",
    "print(\"Simple_RNN Score ---> \", RNN_model.evaluate(test_data, test_labels, verbose=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 33ms/step\n",
      "Sentiment Prediction: [[0.92111695]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nif(prediction < 0.4):\\n    predicted_label = 0\\nelif(prediction >= 0.4 and prediction < 0.7):\\n    predicted_label = 1\\nelse:\\n    predicted_label = 2\\n\\n\\n\\n# Display the binary label\\nprint(\"Predicted Label:\", predicted_label)'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_text = \"This movie was a complete disappointment. The plot was confusing, the characters were poorly developed, and the acting was terrible. I regret wasting my time and money on this film. Definitely not recommended.\"\n",
    "\n",
    "\n",
    "\n",
    "# Preprocess the new text\n",
    "new_text_sequence = tokenizer.texts_to_sequences([new_text])\n",
    "new_text_padded = sequence.pad_sequences(new_text_sequence, maxlen=max_words)\n",
    "\n",
    "# Make the prediction\n",
    "prediction = RNN_model.predict(new_text_padded)\n",
    "\n",
    "# Display the prediction\n",
    "print(\"Sentiment Prediction:\", prediction)\n",
    "\n",
    "# Convert the prediction to label 0(negative), 1(neutral), 2(positive)\n",
    "#predicted_label = 1 if prediction > 0.4 else 0\n",
    "\n",
    "'''\n",
    "if(prediction < 0.4):\n",
    "    predicted_label = 0\n",
    "elif(prediction >= 0.4 and prediction < 0.7):\n",
    "    predicted_label = 1\n",
    "else:\n",
    "    predicted_label = 2\n",
    "\n",
    "\n",
    "\n",
    "# Display the binary label\n",
    "print(\"Predicted Label:\", predicted_label)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
