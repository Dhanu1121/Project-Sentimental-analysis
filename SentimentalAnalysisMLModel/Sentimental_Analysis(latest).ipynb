{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./preprocessed_twitter_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200000, 3)\n",
      "        Unnamed: 0                                              tweet  \\\n",
      "0                0  upset updat facebook text might cri result sch...   \n",
      "1                1       dive mani time ball manag save rest go bound   \n",
      "2                2                    whole bodi feel itchi like fire   \n",
      "3                3                                      behav mad see   \n",
      "4                4                                         whole crew   \n",
      "...            ...                                                ...   \n",
      "199995      199995                                               work   \n",
      "199996      199996      common crash find delet process ok eat memori   \n",
      "199997      199997                    babi boy wear big boy underwear   \n",
      "199998      199998                           fml forgot phone charger   \n",
      "199999      199999  believ wait anoth month phone contract end bor...   \n",
      "\n",
      "        sentiment  \n",
      "0               0  \n",
      "1               0  \n",
      "2               0  \n",
      "3               0  \n",
      "4               0  \n",
      "...           ...  \n",
      "199995          0  \n",
      "199996          0  \n",
      "199997          0  \n",
      "199998          0  \n",
      "199999          0  \n",
      "\n",
      "[200000 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "negative_tweets = df[df['sentiment'] == 0].head(200000)\n",
    "print(negative_tweets.shape)\n",
    "print(negative_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200000, 3)\n",
      "        Unnamed: 0                                              tweet  \\\n",
      "799999      799999                                  love u guy r best   \n",
      "800000      800000      im meet one besti tonight cant wait girl talk   \n",
      "800001      800001  thank twitter add sunisa got meet hin show dc ...   \n",
      "800002      800002  sick realli cheap hurt much eat real food plu ...   \n",
      "800003      800003                                     effect everyon   \n",
      "...            ...                                                ...   \n",
      "999994      999994                                         thank need   \n",
      "999995      999995                                               mayb   \n",
      "999996      999996                 hell window price rang unless free   \n",
      "999997      999997            neah wish reminisc read post last tweet   \n",
      "999998      999998  way rewatch sun goddess last night sasha amaz ...   \n",
      "\n",
      "        sentiment  \n",
      "799999          1  \n",
      "800000          1  \n",
      "800001          1  \n",
      "800002          1  \n",
      "800003          1  \n",
      "...           ...  \n",
      "999994          1  \n",
      "999995          1  \n",
      "999996          1  \n",
      "999997          1  \n",
      "999998          1  \n",
      "\n",
      "[200000 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "positive_tweets = df[df['sentiment'] == 1].head(200000)\n",
    "print(positive_tweets.shape)\n",
    "print(positive_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 3)\n",
      "         Unnamed: 0                                              tweet  \\\n",
      "3123980     3123980                              top tablet damn right   \n",
      "3123981     3123981              cnbctv appl margin better expect aapl   \n",
      "3123987     3123987  rt bought store pretti good logo match wait in...   \n",
      "3124000     3124002  latest appl product lead effici iphon ipad plu...   \n",
      "3124005     3124007                              rt thank think upgrad   \n",
      "...             ...                                                ...   \n",
      "3134336     3134418               rural land reform upset china villag   \n",
      "3134337     3134419  scottish salmon produc say brexit cost million...   \n",
      "3134338     3134420  senat approv defens polici bill includ trump s...   \n",
      "3134339     3134421  sgx singapor exchang metropolitan area employ ...   \n",
      "3134340     3134422            south africa plan duti poultri u brazil   \n",
      "\n",
      "         sentiment  \n",
      "3123980          2  \n",
      "3123981          2  \n",
      "3123987          2  \n",
      "3124000          2  \n",
      "3124005          2  \n",
      "...            ...  \n",
      "3134336          2  \n",
      "3134337          2  \n",
      "3134338          2  \n",
      "3134339          2  \n",
      "3134340          2  \n",
      "\n",
      "[5000 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "neutral_tweets = df[df['sentiment'] == 2].head(5000)\n",
    "print(neutral_tweets.shape)\n",
    "print(neutral_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split negative_tweets into train and test sets\n",
    "train_negative_tweets, test_negative_tweets = train_test_split(negative_tweets, test_size=0.3, random_state=42)\n",
    "\n",
    "# Split positive_tweets into train and test sets\n",
    "train_positive_tweets, test_positive_tweets = train_test_split(positive_tweets, test_size=0.3, random_state=42)\n",
    "\n",
    "# Split neutral_tweets into train and test sets\n",
    "train_neutral_tweets, test_neutral_tweets = train_test_split(neutral_tweets, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\Final Year Project\\Project\\.venv\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Combine the train sets for all sentiment categories and shuffle\n",
    "train_tweets = shuffle(pd.concat([train_negative_tweets, train_positive_tweets, train_neutral_tweets], ignore_index=True), random_state=42)\n",
    "\n",
    "# Combine the test sets for all sentiment categories and shuffle\n",
    "test_tweets = shuffle(pd.concat([test_negative_tweets, test_positive_tweets, test_neutral_tweets], ignore_index=True), random_state=42)\n",
    "\n",
    "# Handle NaN values in the 'text' column\n",
    "train_tweets = train_tweets.dropna(subset=['tweet'])\n",
    "test_tweets = test_tweets.dropna(subset=['tweet'])\n",
    "\n",
    "# Get the normalized text reviews from the combined train and test sets\n",
    "tweet_train_reviews = train_tweets['tweet'].tolist()\n",
    "tweet_test_reviews = test_tweets['tweet'].tolist()\n",
    "\n",
    "# Train lables and test lables of tweets\n",
    "train_labels = train_tweets['sentiment']\n",
    "test_labels = test_tweets['sentiment']\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# Fit the tokenizer on the training data\n",
    "tokenizer.fit_on_texts(tweet_train_reviews)\n",
    "\n",
    "# Convert text to sequences\n",
    "train_sequences = tokenizer.texts_to_sequences(tweet_train_reviews)\n",
    "test_sequences = tokenizer.texts_to_sequences(tweet_test_reviews)\n",
    "\n",
    "# Ensure sequences are padded to the same length\n",
    "maxlen = max(len(seq) for seq in train_sequences + test_sequences)\n",
    "train_sequences = pad_sequences(train_sequences, maxlen=maxlen)\n",
    "test_sequences = pad_sequences(test_sequences, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\Final Year Project\\Project\\.venv\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\Final Year Project\\Project\\.venv\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/5\n",
      "WARNING:tensorflow:From d:\\Final Year Project\\Project\\.venv\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\Final Year Project\\Project\\.venv\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "3528/3528 [==============================] - 207s 58ms/step - loss: 0.5211 - accuracy: 0.7496 - val_loss: 0.4907 - val_accuracy: 0.7675\n",
      "Epoch 2/5\n",
      "3528/3528 [==============================] - 217s 62ms/step - loss: 0.4328 - accuracy: 0.8017 - val_loss: 0.4984 - val_accuracy: 0.7642\n",
      "Epoch 3/5\n",
      "3528/3528 [==============================] - 214s 61ms/step - loss: 0.3720 - accuracy: 0.8343 - val_loss: 0.5299 - val_accuracy: 0.7584\n",
      "Epoch 4/5\n",
      "3528/3528 [==============================] - 218s 62ms/step - loss: 0.3152 - accuracy: 0.8608 - val_loss: 0.5907 - val_accuracy: 0.7428\n",
      "Epoch 5/5\n",
      "3528/3528 [==============================] - 231s 65ms/step - loss: 0.2685 - accuracy: 0.8813 - val_loss: 0.6631 - val_accuracy: 0.7443\n",
      "3782/3782 [==============================] - 18s 5ms/step - loss: 0.6574 - accuracy: 0.7472\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6573936939239502, 0.747217059135437]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# Assuming you have already defined vocab_size, embedding_dim, maxlen, train_sequences, and train_labels\n",
    "# ...\n",
    "vocab_size = len(tokenizer.word_index) + 1  # add 1 for the padding token\n",
    "embedding_dim = 100  # adjust as needed\n",
    "maxlen = maxlen  # as defined earlier\n",
    "train_sequences = train_sequences  # as defined earlier\n",
    "train_labels = train_labels  # as defined earlier\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=maxlen))\n",
    "model.add(LSTM(units=100))\n",
    "model.add(Dense(units=3, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "# Train the model\n",
    "model.fit(train_sequences, train_labels, epochs=5, batch_size=64, validation_split=0.2)\n",
    "model.evaluate(test_sequences, test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Final Year Project\\Project\\.venv\\lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.save('sentiment_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the saved model\n",
    "loaded_model = load_model('sentiment_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 21ms/step\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "new_text_data = \"Today is an average day. Nothing particularly exciting or noteworthy happened, but at least it wasn't a bad day either. #neutral #day\"\n",
    "\n",
    "\n",
    "new_text_sequences = tokenizer.texts_to_sequences([new_text_data])\n",
    "new_text_sequences = pad_sequences(new_text_sequences, maxlen=maxlen)\n",
    "\n",
    "# Make predictions\n",
    "predictions = loaded_model.predict(new_text_sequences)\n",
    "\n",
    "# Assuming a classification task with three classes (adjust accordingly)\n",
    "predicted_class = predictions.argmax(axis=-1)[0]\n",
    "\n",
    "print(predicted_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
